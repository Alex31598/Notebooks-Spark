{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f949b17b8f2814baa87c674deefe4d3",
     "grade": false,
     "grade_id": "cell-f8987996be9f1238",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# La organización Kiva de préstamos entre particulares\n",
    "\n",
    "### Disponible en Kaggle en:\n",
    "https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding/version/5\n",
    "\n",
    "El conjunto de datos elegido recoge estadísticas de Kiva entre 2014 y 2017. Kiva abre un nuevo mundo de oportunidades para los menos favorecidos y nos permite a cualquiera de nosotros convertirnos en superhéroes. Se trata de una organización sin ánimo de lucro que ofrece pequeños préstamos para ayudar a las comunidades desatendidas que no tienen acceso a los servicios bancarios normales. Proporciona una plataforma y une a personas que estén dispuestas a prestar, un mínimo de 25 dólares, y prestatarios que expongan sus necesidades, la finalidad del dinero y las condiciones de devolución,  porque no es una donación, es un préstamo.\n",
    "\n",
    "Al crear este servicio, Kiva habilita una solución que desbloquea capital para todos y mantiene un interés financiero muy bajo para los prestatarios. Por otro lado, permite que cualquiera sea parte de la solución y brinda a las personas un amplio abanico de opciones para elegir quién, dónde, cuánto y para qué sector desea ayudar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a6b4dc108ddf890c659e33701965428",
     "grade": false,
     "grade_id": "cell-f74d7bfd01811789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Variables y significado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea569f4edabc5d7306f8ee2b35adc403",
     "grade": false,
     "grade_id": "cell-9cfb34982bd4eb04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Las variables utilizadas para describir cada préstamo son:\n",
    "\n",
    "\n",
    "* id: ID único para préstamo\n",
    "* funding_amount: la cantidad desembolsada por Kiva al agente de campo (USD)\n",
    "* loan_amount: la cantidad desembolsada por el agente de campo al prestatario (USD)\n",
    "* country_code: código ISO del país en el que se desembolsó el préstamo\n",
    "* activity: Categoría más granular\n",
    "* sector: Categoría de alto nivel\n",
    "* use: Uso exacto del monto del préstamo\n",
    "* country: Nombre completo del país en el que se desembolsó el préstamo\n",
    "* region: Nombre completo de la región dentro del país\n",
    "* currency: La moneda en que se desembolsó el préstamo\n",
    "* partner_id: ID de la organización asociada\n",
    "* posted_time: Hora a la que el agente de campo (intermediario) publica el préstamo en Kiva\n",
    "* disbursed_time: Hora en que el agente de campo (intermediario) entrega el préstamo al beneficiario\n",
    "* funded_time: El momento en que el préstamo publicado en Kiva es financiado por los prestamistas por completo\n",
    "* term_in_months: La duración por la cual el préstamo se desembolsó en meses\n",
    "* lender_count: El número total de prestamistas que contribuyeron a este préstamo.\n",
    "* tags: Etiquetas para describir el caso específico\n",
    "* borrower_genders: letras M, F separadas por comas, donde cada instancia representa un solo hombre / mujer en el grupo\n",
    "* repayment_interval: Estado de pago\n",
    "* date: fecha en la base de datos de esta operación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre completo del alumno:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76dc5b331cac3113e9e77522358617bf",
     "grade": false,
     "grade_id": "cell-b4f9c37a2b92d2e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**INSTRUCCIONES**: en cada celda debes responder a la pregunta formulada, asegurándote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del cálculo quede guardado exactamente en la variable que venía inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). **No olvides borrar la línea *raise NotImplementedError()* de cada celda cuando hayas completado la solución de esa celda y quieras probarla**.\n",
    "\n",
    "Después de cada celda evaluable verás una celda con código. Ejecútala (no modifiques su código) y te dirá si tu solución es correcta o no. En caso de ser correcta, se ejecutará correctamente y no mostrará nada, pero si no lo es mostrará un error. Además de esas pruebas, se realizarán algunas más (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la solución correcta o no. Asegúrate de que, al menos, todas las celdas indican que el código es correcto antes de enviar el notebook terminado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d51114ba5e01cc1f49401e7ec3fec2e",
     "grade": false,
     "grade_id": "cell-69ec0993eeaff3ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Sobre el dataset kiva_loans.csv se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 punto)** Ejercicio 1\n",
    "\n",
    "* Leerlo **sin intentar** que Spark infiera el tipo de dato de cada columna\n",
    "* Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente **si al leer le indicamos las siguientes opciones adicionales** además de las que ya sueles usar: `.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")`.\n",
    "* Asegúrate de que las **filas que no tienen el formato correcto sean descartadas**, indicando también la opción `mode` con el valor `DROPMALFORMED` como vimos en clase.\n",
    "* Crear un nuevo DF `kivaRawNoNullDF` en el que se hayan eliminado todas las filas que tengan algún valor nulo en cualquier columna **excepto en la columna tags**, que no será relevante para el análisis y por tanto podemos ignorar sus valores nulos y mantener dichas filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"ModeloD\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dd7b5af2400b59ee2155e84fbd2bc1b",
     "grade": false,
     "grade_id": "read_csv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'funded_amount', 'loan_amount', 'activity', 'sector', 'use', 'country_code', 'country', 'region', 'currency', 'partner_id', 'posted_time', 'disbursed_time', 'funded_time', 'term_in_months', 'lender_count', 'borrower_genders', 'repayment_interval', 'date']\n"
     ]
    }
   ],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n",
    "kivaRawDF = spark.read\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")\\\n",
    "                 .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                 .csv(\"ModeloD kiva_loans.csv\")\\\n",
    "                 .cache()\n",
    "# Descomentar estas líneas para calcular la lista de columnas que sí deben tenerse en cuenta para quitar nulos. Después\n",
    "# tendrás que utilizar dicha lista en la operación que elimina los nulos\n",
    "columnasExceptoTags = kivaRawDF.columns\n",
    "columnasExceptoTags.remove(\"tags\")\n",
    "print(columnasExceptoTags)\n",
    "kivaRawNoNullDF = kivaRawDF.na.drop(subset = columnasExceptoTags)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "188c34c4f8b30b2afe579969e84c4636",
     "grade": true,
     "grade_id": "read_csv_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "assert(kivaRawNoNullDF.count() == 574115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2298099dcc1ad4426ab91db020be936",
     "grade": false,
     "grade_id": "cell-b90f5b934eda250e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 2\n",
    "\n",
    "* Las columnas `posted_time` y `disbursed_time` son en realidad instantes de tiempo que Spark debería procesar como timestamp. Partiendo de `kivaRawNoNullDF`, reemplaza **ambas columnas** por su versión convertida a timestamp, utilizando `withColumn` con el mismo nombre de cada columna, y donde el nuevo valor de la columna viene dado por el siguiente código:\n",
    "\n",
    "        F.from_unixtime(F.unix_timestamp('nombreColumna', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"))\n",
    "\n",
    "* Además, convierte a `DoubleType` la columna `loan_amount` y a `IntegerType` la columna `term_in_months`.\n",
    "\n",
    "* El DF resultante de todas estas operaciones debe quedar almacenado en la variable `kivaDF`, **cacheado**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca99506697f9a967d82bac2f35307d4d",
     "grade": false,
     "grade_id": "convert_timestamp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import IntegerType\n",
    "# LÍNEAS EVALUABLES, NO RENOMBRAR LAS VARIABLES\n",
    "kivaDF = kivaRawNoNullDF.withColumn(\"posted_time\", \n",
    "                                    F.from_unixtime(F.unix_timestamp('posted_time', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"))\\\n",
    "                        .withColumn(\"disbursed_time\", \n",
    "                                    F.from_unixtime(F.unix_timestamp('disbursed_time', 'yyyy-MM-dd HH:mm:ssXXX')).cast(\"timestamp\"))\\\n",
    "                        .withColumn(\"term_in_months\",\n",
    "                                            F.col(\"term_in_months\").cast(IntegerType()))\\\n",
    "                        .withColumn(\"loan_amount\",\n",
    "                                            F.col(\"loan_amount\").cast(DoubleType()))\\\n",
    "                        .cache()\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bf617944fb4248cb79632c80062746c",
     "grade": true,
     "grade_id": "convert_timestamp_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "typesDict = dict(kivaDF.dtypes)\n",
    "assert(typesDict[\"posted_time\"] == \"timestamp\") \n",
    "assert(typesDict[\"disbursed_time\"] == \"timestamp\") \n",
    "assert(typesDict[\"loan_amount\"] == \"double\") \n",
    "assert(typesDict[\"term_in_months\"] == \"int\")\n",
    "cnt_cond = lambda cond: F.sum(F.when(cond, 1).otherwise(0))\n",
    "nullsRow = kivaDF.select(cnt_cond(F.col(\"posted_time\").isNull()).alias(\"posted_nulls\"),\n",
    "              cnt_cond(F.col(\"disbursed_time\").isNull()).alias(\"disbursed_nulls\")).head()\n",
    "assert(nullsRow.posted_nulls == 0)\n",
    "assert(nullsRow.disbursed_nulls == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e93f385f93838abff6492567725cfba",
     "grade": false,
     "grade_id": "cell-fc88821f19453a51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 puntos)** Ejercicio 3\n",
    "\n",
    "Partiendo de `kivaDF`:\n",
    "\n",
    "* Primero, añade una nueva columna `dias_desembolso` que contenga el número de días que han pasado entre la fecha en que los prestamistas aceptaron financiar un proyecto, y la fecha en que el agente de campo entregó los fondos al beneficiario. Para ello, utiliza `withColumn` en combinación con la función `F.datediff(\"colFuturo\", \"colPasado\")`\n",
    "* De manera análoga, añade otra nueva columna `dias_aceptacion` que contenga el número de días entre el anuncio de la necesidad de préstamo y la aceptación de financiarlo por parte de algún prestamista.\n",
    "* Reemplazar la columna `sector` por otra en la que se hayan traducido las categorías \"Education\" por \"Educacion\" (sin tilde para evitar posibles problemas) y \"Agriculture\" por \"Agricultura\", dejando como están el resto de categorías. **La sustitución no debe tener más que tres casos**: uno para cada categoría que vamos a reemplazar, y un tercero para el resto de categorías, que deben quedarse como estaban.\n",
    "* El resultado debe quedar guardado en la variable `kivaTiemposDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ca4098e1a7f6a6f74ba37e09dc7fafe",
     "grade": false,
     "grade_id": "aniade_tiempos",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "# imports......\n",
    "kivaTiemposDF = kivaDF.withColumn(\"dias_desembolso\", F.days(F.datediff(\"disbursed_time\", \"funded_time\")))\\\n",
    "                      .withColumn(\"dias_aceptacion\", F.days(F.datediff(\"funded_time\", \"posted_time\")))\\\n",
    "                      .withColumn(\"sector\", F.when(F.col(\"sector\") == \"Education\", \"Educacion\")\\\n",
    "                                             .when(F.col(\"sector\") == \"Agriculture\", \"Agricultura\")\\\n",
    "                                             .otherwise(F.col(\"sector\")))\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dcadcc48648f84d47868016a5a1b2176",
     "grade": true,
     "grade_id": "aniade_tiempos_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(kivaTiemposDF.where(\"sector == 'Agricultura'\").count() == 157003)\n",
    "assert(kivaTiemposDF.where(\"sector == 'Educacion'\").count() == 28417)\n",
    "# Comprobamos que las 13 restantes se mantienen sin cambios\n",
    "assert(kivaTiemposDF.groupBy(\"sector\").count().join(kivaDF.groupBy(\"sector\").count(), [\"sector\", \"count\"]).count() == 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79185ceef173a0104b5f923e4bde05a0",
     "grade": false,
     "grade_id": "cell-a71a6b17b1e0d613",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(3 puntos)** Ejercicio 4\n",
    "\n",
    "Partiendo de `kivaTiemposDF`, crear un nuevo DataFrame llamado `kivaAgrupadoDF` que tenga:\n",
    "\n",
    "* Tantas filas como **países (`country`; no usar el código de país)**, y tantas columnas como **sectores** (cada una llamada como el sector) más una (la columna del país, que debe aparecer en primer lugar). En cada celda deberá ir el número **medio (redondeado a 2 cifras decimales)** de días transcurridos en ese país y sector *entre la fecha en que se anuncia la necesidad de préstamo y la fecha en la que un prestamista acepta financiarlo*. Esta columna ha sido calculada en la celda precedente.\n",
    "* Después de esto, añadir una columna adicional `transcurrido_global` con el número **medio (redondeado a 2 cifras decimales) de días transcurridos en cada país** entre ambas fechas *sin tener en cuenta el sector*. Cada fila tendrá la media de las 15 columnas del apartado anterior.\n",
    "* Por último, ordenar el DF resultante **descendentemente** en base al tiempo medio global, `transcurrido_global`. El DF resultado de la ordenación debe ser almacenado en la variable `kivaAgrupadoDF`. \n",
    "\n",
    "PISTA: utiliza el método `pivot` con el sector para el primer apartado, envolviendo a la función de agregación con la función `F.round`, es decir, `F.round(F.funcionAgregacion(....), 2)`, y `withColumn` con una operación aritmética entre columnas en el segundo. **No debe utilizarse la función `when`** ya que Spark es capaz de hacer directamente aritmética entre objetos columna. La operación aritmética también debe estar envuelta por round: `F.round(op. aritmética entre objetos columna, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "396a2289c6c4fa8c5de5540783d27942",
     "grade": false,
     "grade_id": "tiempos_medios",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "kivaAgrupadoDF = kivaTiemposDF.groupBy(F.col(\"country\")).agg(F.mean(\"dias_aceptacion\"))\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93ebd38f5b390fbfbd89be06a50c6282",
     "grade": true,
     "grade_id": "tiempos_medios_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o132.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[country#23], functions=[pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4182, Arts#4183, Clothing#4184, Construction#4185, Educacion#4186, Entertainment#4187, Food#4188, Health#4189, Housing#4190, Manufacturing#4191, Personal Use#4192, Retail#4193, Services#4194, Transportation#4195, Wholesale#4196])\n+- Exchange hashpartitioning(country#23, 200), ENSURE_REQUIREMENTS, [id=#350]\n   +- HashAggregate(keys=[country#23], functions=[partial_pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4165, Arts#4166, Clothing#4167, Construction#4168, Educacion#4169, Entertainment#4170, Food#4171, Health#4172, Housing#4173, Manufacturing#4174, Personal Use#4175, Retail#4176, Services#4177, Transportation#4178, Wholesale#4179])\n      +- *(2) HashAggregate(keys=[country#23, sector#1660], functions=[avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, round(avg(`dias_aceptacion`), 2)#4149])\n         +- Exchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n            +- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n               +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n                  +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n                        +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                              +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                                 +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                                    +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                                          +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3532)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3529)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(country#23, 200), ENSURE_REQUIREMENTS, [id=#350]\n+- HashAggregate(keys=[country#23], functions=[partial_pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4165, Arts#4166, Clothing#4167, Construction#4168, Educacion#4169, Entertainment#4170, Food#4171, Health#4172, Housing#4173, Manufacturing#4174, Personal Use#4175, Retail#4176, Services#4177, Transportation#4178, Wholesale#4179])\n   +- *(2) HashAggregate(keys=[country#23, sector#1660], functions=[avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, round(avg(`dias_aceptacion`), 2)#4149])\n      +- Exchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n         +- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n            +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n               +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n                     +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                           +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                              +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                                 +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                                       +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                             +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:90)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 30 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[country#23], functions=[partial_pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4165, Arts#4166, Clothing#4167, Construction#4168, Educacion#4169, Entertainment#4170, Food#4171, Health#4172, Housing#4173, Manufacturing#4174, Personal Use#4175, Retail#4176, Services#4177, Transportation#4178, Wholesale#4179])\n+- *(2) HashAggregate(keys=[country#23, sector#1660], functions=[avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, round(avg(`dias_aceptacion`), 2)#4149])\n   +- Exchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n      +- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n         +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n            +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n                  +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                           +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                              +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                                    +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                          +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 38 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n+- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n   +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n      +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n            +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                     +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                        +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                              +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                    +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:90)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 50 more\r\nCaused by: java.lang.UnsupportedOperationException: Cannot generate code for expression: days(datediff(cast(input[1, string, true] as date), cast(input[2, timestamp, true] as date)))\r\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:307)\r\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:306)\r\n\tat org.apache.spark.sql.catalyst.expressions.PartitionTransformExpression.doGenCode(PartitionTransforms.scala:35)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\r\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 69 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ALEJAN~1.PER\\AppData\\Local\\Temp/ipykernel_49600/3521668457.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkivaAgrupadoDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountry\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"United States\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgricultura\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m12.0\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAgricultura\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m12.17\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEducacion\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m15.21\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEducacion\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m15.33\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWholesale\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m27.5\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[0;32m   1586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1587\u001b[1;33m             \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1589\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mhead\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1587\u001b[0m             \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1589\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1591\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Alice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Bob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \"\"\"\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o132.collectToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[country#23], functions=[pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4182, Arts#4183, Clothing#4184, Construction#4185, Educacion#4186, Entertainment#4187, Food#4188, Health#4189, Housing#4190, Manufacturing#4191, Personal Use#4192, Retail#4193, Services#4194, Transportation#4195, Wholesale#4196])\n+- Exchange hashpartitioning(country#23, 200), ENSURE_REQUIREMENTS, [id=#350]\n   +- HashAggregate(keys=[country#23], functions=[partial_pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4165, Arts#4166, Clothing#4167, Construction#4168, Educacion#4169, Entertainment#4170, Food#4171, Health#4172, Housing#4173, Manufacturing#4174, Personal Use#4175, Retail#4176, Services#4177, Transportation#4178, Wholesale#4179])\n      +- *(2) HashAggregate(keys=[country#23, sector#1660], functions=[avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, round(avg(`dias_aceptacion`), 2)#4149])\n         +- Exchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n            +- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n               +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n                  +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n                        +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                              +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                                 +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                                    +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                                          +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:439)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3532)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3529)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(country#23, 200), ENSURE_REQUIREMENTS, [id=#350]\n+- HashAggregate(keys=[country#23], functions=[partial_pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4165, Arts#4166, Clothing#4167, Construction#4168, Educacion#4169, Entertainment#4170, Food#4171, Health#4172, Housing#4173, Manufacturing#4174, Personal Use#4175, Retail#4176, Services#4177, Transportation#4178, Wholesale#4179])\n   +- *(2) HashAggregate(keys=[country#23, sector#1660], functions=[avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, round(avg(`dias_aceptacion`), 2)#4149])\n      +- Exchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n         +- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n            +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n               +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n                     +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                           +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                              +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                                 +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                                       +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                             +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:90)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 30 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[country#23], functions=[partial_pivotfirst(sector#1660, round(avg(`dias_aceptacion`), 2)#4149, Agricultura, Arts, Clothing, Construction, Educacion, Entertainment, Food, Health, Housing, Manufacturing, Personal Use, Retail, Services, Transportation, Wholesale, 0, 0)], output=[country#23, Agricultura#4165, Arts#4166, Clothing#4167, Construction#4168, Educacion#4169, Entertainment#4170, Food#4171, Health#4172, Housing#4173, Manufacturing#4174, Personal Use#4175, Retail#4176, Services#4177, Transportation#4178, Wholesale#4179])\n+- *(2) HashAggregate(keys=[country#23, sector#1660], functions=[avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, round(avg(`dias_aceptacion`), 2)#4149])\n   +- Exchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n      +- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n         +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n            +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n                  +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                           +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                              +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                                    +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                          +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:83)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 38 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(country#23, sector#1660, 200), ENSURE_REQUIREMENTS, [id=#345]\n+- *(1) HashAggregate(keys=[country#23, sector#1660], functions=[partial_avg(cast(dias_aceptacion#1637 as bigint))], output=[country#23, sector#1660, sum#4605, count#4606L])\n   +- *(1) Project [CASE WHEN (sector#20 = Education) THEN Educacion WHEN (sector#20 = Agriculture) THEN Agricultura ELSE sector#20 END AS sector#1660, country#23, days(datediff(cast(funded_time#29 as date), cast(posted_time#621 as date))) AS dias_aceptacion#1637]\n      +- InMemoryTableScan [country#23, funded_time#29, posted_time#621, sector#20]\n            +- InMemoryRelation [id#16, funded_amount#17, loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#621, disbursed_time#642, funded_time#29, term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  +- *(1) Project [id#16, funded_amount#17, cast(loan_amount#18 as double) AS loan_amount#684, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, cast(from_unixtime(unix_timestamp(posted_time#27, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS posted_time#621, cast(from_unixtime(unix_timestamp(disbursed_time#28, yyyy-MM-dd HH:mm:ssXXX, Some(Europe/Paris), false), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as timestamp) AS disbursed_time#642, funded_time#29, cast(term_in_months#30 as int) AS term_in_months#663, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35]\n                     +- *(1) Filter AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)\n                        +- InMemoryTableScan [activity#19, borrower_genders#33, country#23, country_code#22, currency#25, date#35, disbursed_time#28, funded_amount#17, funded_time#29, id#16, lender_count#31, loan_amount#18, partner_id#26, posted_time#27, region#24, repayment_interval#34, sector#20, tags#32, term_in_months#30, use#21], [AtLeastNNulls(n, id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,borrower_genders#33,repayment_interval#34,date#35)]\n                              +- InMemoryRelation [id#16, funded_amount#17, loan_amount#18, activity#19, sector#20, use#21, country_code#22, country#23, region#24, currency#25, partner_id#26, posted_time#27, disbursed_time#28, funded_time#29, term_in_months#30, lender_count#31, tags#32, borrower_genders#33, repayment_interval#34, date#35], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                    +- FileScan csv [id#16,funded_amount#17,loan_amount#18,activity#19,sector#20,use#21,country_code#22,country#23,region#24,currency#25,partner_id#26,posted_time#27,disbursed_time#28,funded_time#29,term_in_months#30,lender_count#31,tags#32,borrower_genders#33,repayment_interval#34,date#35] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/alejandro.perez/Documents/NotebooksGithub/Tarea/ModeloD kiva_loa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,funded_amount:string,loan_amount:string,activity:string,sector:string,use:string...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:90)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 50 more\r\nCaused by: java.lang.UnsupportedOperationException: Cannot generate code for expression: days(datediff(cast(input[1, string, true] as date), cast(input[2, timestamp, true] as date)))\r\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:307)\r\n\tat org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:306)\r\n\tat org.apache.spark.sql.catalyst.expressions.PartitionTransformExpression.doGenCode(PartitionTransforms.scala:35)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\r\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:163)\r\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\r\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1026)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\r\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\r\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\r\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\r\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:733)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 69 more\r\n"
     ]
    }
   ],
   "source": [
    "r1 = kivaAgrupadoDF.head()\n",
    "assert(r1.country == \"United States\")\n",
    "assert((r1.Agricultura - 12.0 < 0.01) | (r1.Agricultura - 12.17 < 0.01))\n",
    "assert((r1.Educacion - 15.21 < 0.01) | (r1.Educacion - 15.33 < 0.01))\n",
    "assert(r1.Wholesale - 27.5 < 0.01)\n",
    "assert((r1.transcurrido_global - 20.94 < 0.01) | (r1.transcurrido_global - 21.04 < 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31924076e8ce9e4467959d6eef2edcc4",
     "grade": false,
     "grade_id": "cell-c5ec05706eccd480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(3 puntos)** Ejercicio 5\n",
    "\n",
    "Partiendo de nuevo de `kivaTiemposDF`, añadir las siguientes columnas:\n",
    "\n",
    "* Primero, tres columnas adicionales llamadas `transc_medio`, `transc_min`, `transc_max` que contengan, respectivamente, **el número de días medio, mínimo y máximo transcurrido para proyectos de ese mismo país y ese mismo sector** entre la fecha en que se postea la necesidad de préstamo y la fecha en la que alguien acepta financiarlo (es decir, la columna `dias_aceptacion` calculada antes y utilizada también en la celda anterior). Es decir, queremos una columna extra para que podamos tener, junto a cada préstamo, información agregada de los préstamos similares, entendidos como aquellos del mismo país y del mismo sector. **No se debe utilizar JOIN sino solo funciones de ventana**.\n",
    "* Finalmente, crear otra columna adicional `diff_dias` que contenga la **diferencia en días entre los días que transcurrieron en este proyecto y la media de días de los proyectos similares** (calculada en el apartado anterior). Debería ser lo primero menos lo segundo, de forma que un número positivo indica que este préstamo tardó más días en ser aceptado que la media de días de este país y sector, y un número negativo indica lo contrario. El resultado debe obtenerse aplicando operaciones aritméticas con columnas existentes, **sin utilizar `when`**.\n",
    "* El DF resultante con las 4 columnas nuevas que hemos añadido debe quedar almacenado en la variable `kivaExtraInfoDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7781e45de12eaece0b66e4eb898e2b0b",
     "grade": false,
     "grade_id": "ventana",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "# imports necesarios..........\n",
    "windowPaisSector = None\n",
    "kivaExtraInfoDF = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "844f128aa6e789cd3660a00ab0159b56",
     "grade": true,
     "grade_id": "ventana_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r = kivaExtraInfoDF.where(\"id = '658540'\").head()\n",
    "assert(r.country == 'Burkina Faso')\n",
    "assert(r.transc_medio - 11.02 < 0.05)\n",
    "assert(r.dias_aceptacion == 35)\n",
    "assert(r.diff_dias - 24.0 < 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
